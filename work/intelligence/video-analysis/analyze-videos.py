#!/usr/bin/env python3
"""
AI-Driven Video Analysis Pipeline
Automated analysis of Luoxi incident videos
Generated by CEO Agent - Phase 1.5
"""

import asyncio
import subprocess
import json
import os
import base64
import glob
from pathlib import Path
from datetime import datetime
import anthropic
import whisper

# Configuration
VIDEO_DIR = Path("../../../media")
OUTPUT_DIR = Path(".")
FRAMES_DIR = OUTPUT_DIR / "frames"
AUDIO_DIR = OUTPUT_DIR / "audio"
METADATA_DIR = OUTPUT_DIR / "metadata"
TRANSCRIPTS_DIR = OUTPUT_DIR / "transcripts"
ANALYSIS_DIR = OUTPUT_DIR / "frame-analysis"
REPORTS_DIR = OUTPUT_DIR / "reports"

class VideoAnalysisPipeline:
    def __init__(self):
        self.anthropic_client = None
        self.whisper_model = None
        self.videos_processed = []
        self.total_cost = 0.0

    def init_apis(self):
        """Initialize API clients"""
        print("\n[Init] Initializing APIs...")

        # Anthropic Claude
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable not set")
        self.anthropic_client = anthropic.Anthropic(api_key=api_key)
        print("✓ Claude API initialized")

        # Whisper
        print("Loading Whisper model (this may take a minute)...")
        self.whisper_model = whisper.load_model("large")  # Best for Chinese
        print("✓ Whisper model loaded")

    def find_videos(self):
        """Find all video files"""
        print("\n[1/5] Finding video files...")

        video_extensions = ['.mp4', '.mov', '.avi', '.mkv']
        videos = []

        for ext in video_extensions:
            videos.extend(glob.glob(str(VIDEO_DIR / f"*{ext}")))

        print(f"✓ Found {len(videos)} video files")
        return videos

    def extract_frames_and_audio(self, video_path):
        """Extract frames and audio from video"""
        video_name = Path(video_path).stem
        print(f"\n  → Extracting frames and audio: {video_name}")

        # Create output directories
        video_frames_dir = FRAMES_DIR / video_name
        video_frames_dir.mkdir(parents=True, exist_ok=True)

        audio_file = AUDIO_DIR / f"{video_name}.wav"
        metadata_file = METADATA_DIR / f"{video_name}.json"

        # Extract frames (1 per second)
        try:
            subprocess.run([
                'ffmpeg', '-i', video_path,
                '-vf', 'fps=1',
                '-q:v', '2',  # High quality
                str(video_frames_dir / '%04d.jpg'),
                '-y'  # Overwrite
            ], check=True, capture_output=True, text=True)

            frame_count = len(list(video_frames_dir.glob('*.jpg')))
            print(f"    ✓ Extracted {frame_count} frames")
        except subprocess.CalledProcessError as e:
            print(f"    ✗ Frame extraction failed: {e.stderr}")
            frame_count = 0

        # Extract audio
        try:
            subprocess.run([
                'ffmpeg', '-i', video_path,
                '-vn',  # No video
                '-acodec', 'pcm_s16le',
                '-ar', '16000',  # 16kHz for Whisper
                '-ac', '1',  # Mono
                str(audio_file),
                '-y'
            ], check=True, capture_output=True, text=True)
            print(f"    ✓ Extracted audio: {audio_file.name}")
        except subprocess.CalledProcessError as e:
            print(f"    ✗ Audio extraction failed: {e.stderr}")
            audio_file = None

        # Extract metadata
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet',
                '-print_format', 'json',
                '-show_format', '-show_streams',
                video_path
            ], check=True, capture_output=True, text=True)

            metadata = json.loads(result.stdout)
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"    ✓ Extracted metadata")
        except Exception as e:
            print(f"    ✗ Metadata extraction failed: {e}")
            metadata = {}

        return {
            'video_name': video_name,
            'frames_dir': video_frames_dir,
            'frame_count': frame_count,
            'audio_file': audio_file,
            'metadata': metadata
        }

    def transcribe_audio(self, audio_file, video_name):
        """Transcribe Chinese audio using Whisper"""
        print(f"\n  → Transcribing audio: {video_name}")

        if not audio_file or not audio_file.exists():
            print(f"    ✗ Audio file not found")
            return None

        try:
            result = self.whisper_model.transcribe(
                str(audio_file),
                language="zh",  # Chinese
                task="transcribe",
                verbose=False
            )

            transcript_data = {
                'video_name': video_name,
                'text': result['text'],
                'language': result['language'],
                'segments': [
                    {
                        'start': seg['start'],
                        'end': seg['end'],
                        'text': seg['text']
                    }
                    for seg in result['segments']
                ]
            }

            # Save transcript
            transcript_file = TRANSCRIPTS_DIR / f"{video_name}_transcript.json"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)

            # Calculate cost (Whisper API pricing: ~$0.006/minute)
            duration = result['segments'][-1]['end'] if result['segments'] else 0
            cost = (duration / 60) * 0.006
            self.total_cost += cost

            print(f"    ✓ Transcribed {len(result['segments'])} segments")
            print(f"    ✓ Duration: {duration:.1f}s | Cost: ${cost:.3f}")

            return transcript_data

        except Exception as e:
            print(f"    ✗ Transcription failed: {e}")
            return None

    def analyze_frames(self, frames_dir, video_name, transcript=None):
        """Analyze frames using Claude API"""
        print(f"\n  → Analyzing frames with Claude: {video_name}")

        frame_files = sorted(frames_dir.glob('*.jpg'))
        if not frame_files:
            print(f"    ✗ No frames found")
            return None

        # Sample frames (every 5th frame to reduce API calls)
        sampled_frames = frame_files[::5][:50]  # Max 50 frames

        print(f"    Analyzing {len(sampled_frames)} frames...")

        # Prepare frame data for Claude
        frame_content = []
        for frame_file in sampled_frames:
            with open(frame_file, 'rb') as f:
                image_data = base64.standard_b64encode(f.read()).decode('utf-8')
                frame_content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    }
                })

        # Build prompt
        prompt = f"""Analyze these video frames from the Luoxi medical incident.

Video: {video_name}
Frames analyzed: {len(sampled_frames)} (sampled from {len(frame_files)} total frames)
"""

        if transcript:
            prompt += f"\nAudio transcript available:\n{transcript['text'][:500]}...\n"

        prompt += """
For each emotionally significant moment, identify:

1. **Timestamp** (based on frame number in filename)
2. **Visual content** (what's happening)
3. **Emotional impact** (grief/anger/tenderness/injustice/protest)
4. **Viral potential** (1-10 score with reasoning)
5. **Platform fit** (TikTok/Instagram/YouTube - why?)
6. **Chinese text visible** (transcribe any visible text)
7. **Key moments** (specific timestamps of powerful clips)
8. **Production notes** (how to use this in content)

Focus on:
- Family grief moments
- Protests and confrontations
- Evidence documents shown
- Symbolic moments (empty clothes, feeding rituals, etc.)
- Police/authority interactions
- Public support demonstrations

Be specific with timestamps and provide actionable recommendations for viral content production.
"""

        # Call Claude API
        try:
            message = self.anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=8192,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        *frame_content
                    ]
                }]
            )

            analysis_text = message.content[0].text

            # Calculate cost (Claude Sonnet 4: ~$0.024 per image)
            image_cost = len(sampled_frames) * 0.024
            self.total_cost += image_cost

            print(f"    ✓ Analysis complete")
            print(f"    ✓ Images processed: {len(sampled_frames)} | Cost: ${image_cost:.2f}")

            # Save analysis
            analysis_data = {
                'video_name': video_name,
                'frames_analyzed': len(sampled_frames),
                'frames_total': len(frame_files),
                'analysis': analysis_text,
                'transcript_included': transcript is not None,
                'cost': image_cost
            }

            analysis_file = ANALYSIS_DIR / f"{video_name}_analysis.json"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)

            # Also save as markdown for easy reading
            md_file = ANALYSIS_DIR / f"{video_name}_analysis.md"
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(f"# Video Analysis: {video_name}\n\n")
                f.write(f"**Frames analyzed:** {len(sampled_frames)} / {len(frame_files)}\n")
                f.write(f"**Has transcript:** {'Yes' if transcript else 'No'}\n\n")
                f.write("---\n\n")
                f.write(analysis_text)

            return analysis_data

        except Exception as e:
            print(f"    ✗ Claude API error: {e}")
            return None

    def process_video(self, video_path):
        """Process single video through full pipeline"""
        video_name = Path(video_path).stem
        print(f"\n{'='*60}")
        print(f"Processing: {video_name}")
        print(f"{'='*60}")

        # Stage 1: Extract frames and audio
        extraction = self.extract_frames_and_audio(video_path)

        # Stage 2: Transcribe audio
        transcript = None
        if extraction['audio_file']:
            transcript = self.transcribe_audio(extraction['audio_file'], video_name)

        # Stage 3: Analyze frames
        analysis = self.analyze_frames(
            extraction['frames_dir'],
            video_name,
            transcript
        )

        # Record results
        self.videos_processed.append({
            'video_name': video_name,
            'video_path': video_path,
            'frame_count': extraction['frame_count'],
            'has_transcript': transcript is not None,
            'has_analysis': analysis is not None,
            'processed_at': datetime.now().isoformat()
        })

    def generate_master_report(self):
        """Generate comprehensive master report"""
        print(f"\n{'='*60}")
        print("Generating Master Report")
        print(f"{'='*60}")

        # Collect all analyses
        all_analyses = []
        for analysis_file in ANALYSIS_DIR.glob('*_analysis.json'):
            with open(analysis_file, 'r', encoding='utf-8') as f:
                all_analyses.append(json.load(f))

        # Collect all transcripts
        all_transcripts = []
        for transcript_file in TRANSCRIPTS_DIR.glob('*_transcript.json'):
            with open(transcript_file, 'r', encoding='utf-8') as f:
                all_transcripts.append(json.load(f))

        # Generate master report
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': {
                'total_videos_processed': len(self.videos_processed),
                'videos_with_transcripts': len(all_transcripts),
                'videos_with_analysis': len(all_analyses),
                'total_cost_estimate': f"${self.total_cost:.2f}"
            },
            'videos': self.videos_processed,
            'transcripts_summary': [
                {
                    'video': t['video_name'],
                    'duration': t['segments'][-1]['end'] if t['segments'] else 0,
                    'segment_count': len(t['segments']),
                    'text_preview': t['text'][:200] + '...' if len(t['text']) > 200 else t['text']
                }
                for t in all_transcripts
            ],
            'analyses_summary': [
                {
                    'video': a['video_name'],
                    'frames_analyzed': a['frames_analyzed'],
                    'frames_total': a['frames_total']
                }
                for a in all_analyses
            ]
        }

        # Save JSON report
        report_file = REPORTS_DIR / "VIDEO-ANALYSIS-MASTER-REPORT.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # Generate markdown report
        md_file = REPORTS_DIR / "VIDEO-ANALYSIS-MASTER-REPORT.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# Video Analysis Master Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            f.write("## Summary\n\n")
            f.write(f"- **Total videos processed:** {len(self.videos_processed)}\n")
            f.write(f"- **Videos with transcripts:** {len(all_transcripts)}\n")
            f.write(f"- **Videos with frame analysis:** {len(all_analyses)}\n")
            f.write(f"- **Total cost:** ${self.total_cost:.2f}\n\n")
            f.write("---\n\n")
            f.write("## Processed Videos\n\n")

            for video in self.videos_processed:
                f.write(f"### {video['video_name']}\n\n")
                f.write(f"- **Frames extracted:** {video['frame_count']}\n")
                f.write(f"- **Transcript:** {'✓ Yes' if video['has_transcript'] else '✗ No'}\n")
                f.write(f"- **Analysis:** {'✓ Yes' if video['has_analysis'] else '✗ No'}\n\n")

            f.write("---\n\n")
            f.write("## Individual Reports\n\n")
            f.write("Detailed analyses available in:\n")
            f.write("- `frame-analysis/` - Frame-by-frame Claude analysis\n")
            f.write("- `transcripts/` - Chinese audio transcripts with timestamps\n")
            f.write("- `frames/` - Extracted frame images\n")
            f.write("- `audio/` - Extracted audio files\n")

        print(f"\n✓ Master report generated: {report_file}")
        print(f"✓ Markdown report: {md_file}")

        return report

    async def run_pipeline(self):
        """Run full pipeline on all videos"""
        print(f"\n{'='*60}")
        print("Video Analysis Pipeline - Starting")
        print(f"{'='*60}")

        start_time = datetime.now()

        # Initialize
        self.init_apis()

        # Find videos
        videos = self.find_videos()

        if not videos:
            print("\n✗ No videos found!")
            return

        # Process each video
        print(f"\n[2/5] Extracting frames and audio from all videos...")
        print(f"[3/5] Transcribing audio (Chinese) with Whisper...")
        print(f"[4/5] Analyzing frames with Claude API...")
        print(f"\nProcessing {len(videos)} videos...\n")

        for i, video_path in enumerate(videos, 1):
            print(f"\n{'='*60}")
            print(f"Video {i}/{len(videos)}")
            print(f"{'='*60}")
            self.process_video(video_path)

            # Rate limiting (be respectful to APIs)
            if i < len(videos):
                await asyncio.sleep(2)

        # Generate master report
        print(f"\n[5/5] Generating master report...")
        self.generate_master_report()

        # Final summary
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\n{'='*60}")
        print("Pipeline Complete!")
        print(f"{'='*60}")
        print(f"Videos processed: {len(self.videos_processed)}")
        print(f"Total time: {duration/60:.1f} minutes")
        print(f"Total cost: ${self.total_cost:.2f}")
        print(f"Output directory: {OUTPUT_DIR}")
        print(f"{'='*60}\n")

async def main():
    """Entry point"""
    pipeline = VideoAnalysisPipeline()
    await pipeline.run_pipeline()

if __name__ == "__main__":
    asyncio.run(main())
