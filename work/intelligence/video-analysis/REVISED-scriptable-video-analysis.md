# REVISED: Scriptable Video Analysis Solution

## Problem with Original Approach
❌ Recommended humans watch all 20 videos manually (8-12 hours)
❌ Not scalable
❌ Not agent-drivable

## Corrected Approach: AI-Driven Video Processing Pipeline

### Solution: Automated Video Analysis Stack

**Tools (all scriptable):**
1. **ffmpeg** - Extract frames, audio, metadata (command line)
2. **Whisper API** - Transcribe Chinese audio to text (OpenAI API)
3. **Claude API** - Analyze extracted frames (API)
4. **Python scripts** - Orchestrate the pipeline (generated by agent)

### Installation (One-time)
```bash
brew install ffmpeg
pip install openai-whisper anthropic
```

### Scriptable Video Analysis Pipeline

#### Stage 1: Automated Frame Extraction
```bash
# Agent generates this script for each video
for video in media/*.mp4; do
  filename=$(basename "$video" .mp4)

  # Extract 1 frame per second
  ffmpeg -i "$video" \
    -vf "fps=1" \
    "work/intelligence/video-analysis/frames/${filename}_%04d.jpg"

  # Extract audio for transcription
  ffmpeg -i "$video" \
    -vn -acodec pcm_s16le -ar 16000 \
    "work/intelligence/video-analysis/audio/${filename}.wav"

  # Extract metadata
  ffprobe -v quiet -print_format json -show_format -show_streams \
    "$video" > "work/intelligence/video-analysis/metadata/${filename}.json"
done
```

**Result:** Each 60-second video → 60 frames + audio file + metadata JSON

#### Stage 2: Automated Audio Transcription (Chinese)
```python
# Agent generates this Python script
import whisper
import glob
import json

model = whisper.load_model("large")  # Best for Chinese

for audio_file in glob.glob("work/intelligence/video-analysis/audio/*.wav"):
    result = model.transcribe(audio_file, language="zh")

    # Save transcript with timestamps
    transcript = {
        "file": audio_file,
        "text": result["text"],
        "segments": [
            {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"]
            }
            for seg in result["segments"]
        ]
    }

    output_file = audio_file.replace(".wav", "_transcript.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(transcript, f, ensure_ascii=False, indent=2)

print("Transcription complete. Transcripts saved to audio/*.json")
```

**Result:** Chinese transcripts with timestamps for all videos

#### Stage 3: Automated Frame Analysis (Claude API)
```python
# Agent generates this Python script
import anthropic
import base64
import glob
import json

client = anthropic.Anthropic()

def analyze_frames(video_name, frame_files):
    """Analyze frames from one video"""

    # Send every 5th frame to Claude (12 frames per minute)
    frames_to_analyze = frame_files[::5]

    # Prepare frame images
    frame_data = []
    for frame_file in frames_to_analyze[:50]:  # Max 50 frames per call
        with open(frame_file, 'rb') as f:
            image_data = base64.standard_b64encode(f.read()).decode('utf-8')
            frame_data.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": image_data
                }
            })

    # Ask Claude to analyze
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": """Analyze these video frames from Luoxi incident footage.

For each emotionally significant moment, identify:
1. Timestamp (based on frame number)
2. What's happening visually
3. Emotional impact (grief/anger/tenderness/injustice)
4. Viral potential (1-10)
5. Platform fit (TikTok/Instagram/YouTube)
6. Any visible Chinese text (transcribe it)

Focus on: family grief, protests, confrontations, evidence documents, symbolic moments."""
                },
                *frame_data
            ]
        }]
    )

    return {
        "video": video_name,
        "analysis": message.content[0].text,
        "frames_analyzed": len(frames_to_analyze)
    }

# Process all videos
for video_dir in glob.glob("work/intelligence/video-analysis/frames/*/"):
    video_name = video_dir.split('/')[-2]
    frame_files = sorted(glob.glob(f"{video_dir}*.jpg"))

    analysis = analyze_frames(video_name, frame_files)

    output_file = f"work/intelligence/video-analysis/frame-analysis/{video_name}_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)

print("Frame analysis complete.")
```

**Result:** Claude's analysis of emotional moments, viral clips, visible text

#### Stage 4: Automated Synthesis
```python
# Agent generates this Python script
import json
import glob

def synthesize_video_intelligence():
    """Combine transcripts + frame analysis + metadata"""

    all_videos = {}

    # Load all data
    for video_name in get_video_list():
        all_videos[video_name] = {
            "metadata": load_json(f"metadata/{video_name}.json"),
            "transcript": load_json(f"audio/{video_name}_transcript.json"),
            "frame_analysis": load_json(f"frame-analysis/{video_name}_analysis.json")
        }

    # Generate master report
    report = {
        "summary": {
            "total_videos": len(all_videos),
            "total_duration": sum_durations(all_videos),
            "top_viral_clips": identify_top_clips(all_videos),
            "key_quotes": extract_quotes(all_videos),
            "translation_priorities": prioritize_translations(all_videos)
        },
        "videos": all_videos
    }

    with open("work/intelligence/video-analysis/VIDEO-INTELLIGENCE-MASTER.json", 'w') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # Generate markdown report for CEO
    generate_markdown_report(report)

synthesize_video_intelligence()
```

**Result:** Complete video intelligence ready for Phase 2 agents

### Execution Workflow

**Agent does:**
1. Generate all Python/bash scripts above (5 minutes)
2. Execute frame extraction via ffmpeg (30 minutes automated)
3. Execute Whisper transcription via API (1-2 hours automated)
4. Execute Claude frame analysis via API (2-3 hours automated)
5. Synthesize results into reports (10 minutes)

**Human does:**
- Run initial setup commands (5 minutes one-time)
- Approve API usage (OpenAI Whisper, Claude API)
- Let scripts run overnight

**Total human time: 10 minutes**
**Total agent-driven processing: 4-6 hours automated**

### Cost Estimate
- **Whisper API:** ~$0.006/minute audio × 60 minutes ≈ $0.36
- **Claude API:** ~50 images × 20 videos × $0.024/image ≈ $24
- **Total:** ~$25 for complete automated analysis

### Why This Is Better

✅ **Fully automated** - no humans watching videos
✅ **Scalable** - works for 20 videos or 200 videos
✅ **Agent-drivable** - agent generates and executes scripts
✅ **API-based** - Whisper + Claude do the heavy lifting
✅ **Comprehensive** - transcripts + frame analysis + metadata
✅ **Production-ready** - outputs JSON + markdown for next phase

### Alternative: All-in-One Tools

If simpler approach needed:
```python
# Use Google Video Intelligence API
from google.cloud import videointelligence

video_client = videointelligence.VideoIntelligenceServiceClient()
features = [
    videointelligence.Feature.LABEL_DETECTION,
    videointelligence.Feature.SHOT_CHANGE_DETECTION,
    videointelligence.Feature.SPEECH_TRANSCRIPTION,
    videointelligence.Feature.TEXT_DETECTION
]

# One API call per video - fully automated
operation = video_client.annotate_video(
    request={"features": features, "input_uri": video_gcs_uri}
)
```

## CEO Decision Required

1. Approve revised scriptable approach?
2. Approve API costs (~$25 for Whisper + Claude)?
3. Alternative: Use Google Video Intelligence API (~$0.10/minute = $6 total)?

**Recommended:** ffmpeg + Whisper + Claude stack (more control, better Chinese transcription)

This is the maximally scriptable, agent-driven solution.
